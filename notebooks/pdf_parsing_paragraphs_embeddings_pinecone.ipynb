{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77b961-a521-45a0-a7bd-0aae88f599e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sentence-transformers\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4884f9-be2e-493b-adb5-077c680f6200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a669a-3907-41b4-ba8c-205f5e394dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4b97980-924e-41be-8e85-9fb51311df6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a5bffb-2f9a-480f-b953-c2bd75534b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"Read from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e81d52b-be0b-439b-b03e-ca67510156a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Executive Summary\\nPart 1: Foundational Information\\n1 Framing Risk Artificial intelligence (AI) technologies have significant potential to transform society and\\npeople’s lives – from commerce and health to transportation and cybersecurity to the envi-\\nronment and our planet. AI technologies can drive inclusive economic growth and support\\nscientific advancements that improve the conditions of our world. AI technologies, how-\\never, also pose risks that can negatively impact individuals, groups, organizations, commu-\\nnities, society, the environment, and the planet. Like risks for other types of technology, AI\\nrisks can emerge in a variety of ways and can be characterized as long- or short-term, high-\\nor low-probability, systemic or localized, and high- or low-impact. The AI RMF refers to an AI system as an engineered or machine-based system that\\ncan, for a given set of objectives, generate outputs such as predictions, recommenda-\\ntions, or decisions influencing real or virtual environments. AI systems are designed\\nto operate with varying levels of autonomy (Adapted from: OECD Recommendation\\non AI:2019; ISO/IEC 22989:2022). While there are myriad standards and best practices to help organizations mitigate the risks\\nof traditional software or information-based systems, the risks posed by AI systems are in\\nmany ways unique (See Appendix B). AI systems, for example, may be trained on data that\\ncan change over time, sometimes significantly and unexpectedly, affecting system function-\\nality and trustworthiness in ways that are hard to understand. AI systems and the contexts\\nin which they are deployed are frequently complex, making it difficult to detect and respond\\nto failures when they occur. AI systems are inherently socio-technical in nature, meaning\\nthey are influenced by societal dynamics and human behavior. AI risks – and benefits –\\ncan emerge from the interplay of technical aspects combined with societal factors related\\nto how a system is used, its interactions with other AI systems, who operates it, and the\\nsocial context in which it is deployed. These risks make AI a uniquely challenging technology to deploy and utilize both for orga-\\nnizations and within society. Without proper controls, AI systems can amplify, perpetuate,\\nor exacerbate inequitable or undesirable outcomes for individuals and communities. With\\nproper controls, AI systems can mitigate and manage inequitable outcomes. AI risk management is a key component of responsible development and use of AI sys-\\ntems. Responsible AI practices can help align the decisions about AI system design, de-\\nvelopment, and uses with intended aim and values. Core concepts in responsible AI em-\\nphasize human centricity, social responsibility, and sustainability. AI risk management can\\ndrive responsible uses and practices by prompting organizations and their internal teams\\nwho design, develop, and deploy AI to think more critically about context and potential\\nor unexpected negative and positive impacts. Understanding and managing the risks of AI\\nsystems will help to enhance trustworthiness, and in turn, cultivate public trust. Page 1 NIST AI 100-1 AI RMF 1.0 Social responsibility can refer to the organization’s responsibility “for the impacts\\nof its decisions and activities on society and the environment through transparent\\nand ethical behavior” (ISO 26000:2010). Sustainability refers to the “state of the\\nglobal system, including environmental, social, and economic aspects, in which the\\nneeds of the present are met without compromising the ability of future generations\\nto meet their own needs” (ISO/IEC TR 24368:2022). Responsible AI is meant to\\nresult in technology that is also equitable and accountable. The expectation is that\\norganizational practices are carried out in accord with “professional responsibility,”\\ndefined by ISO as an approach that “aims to ensure that professionals who design,\\ndevelop, or deploy AI systems and applications or AI-based products or systems,\\nrecognize their unique position to exert influence on people, society, and the future\\nof AI” (ISO/IEC TR 24368:2022). As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283),\\nthe goal of the AI RMF is to offer a resource to the organizations designing, developing,\\ndeploying, or using AI systems to help manage the many risks of AI and promote trustwor-\\nthy and responsible development and use of AI systems. The Framework is intended to be\\nvoluntary, rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-\\nity to organizations of all sizes and in all sectors and throughout society to implement the\\napproaches in the Framework. The Framework is designed to equip organizations and individuals – referred to here as\\nAI actors – with approaches that increase the trustworthiness of AI systems, and to help\\nfoster the responsible design, development, deployment, and use of AI systems over time.\\nAI actors are defined by the Organisation for Economic Co-operation and Development\\n(OECD) as “those who play an active role in the AI system lifecycle, including organiza-\\ntions and individuals that deploy or operate AI” [OECD (2019) Artificial Intelligence in\\nSociety—OECD iLibrary] (See Appendix A). The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies\\ncontinue to develop, and to be operationalized by organizations in varying degrees and\\ncapacities so society can benefit from AI while also being protected from its potential\\nharms. The Framework and supporting resources will be updated, expanded, and improved based\\non evolving technology, the standards landscape around the world, and AI community ex-\\nperience and feedback. NIST will continue to align the AI RMF and related guidance with\\napplicable international standards, guidelines, and practices. As the AI RMF is put into\\nuse, additional lessons will be learned to inform future updates and additional resources. The Framework is divided into two parts. Part 1 discusses how organizations can frame\\nthe risks related to AI and describes the intended audience. Next, AI risks and trustworthi-\\nness are analyzed, outlining the characteristics of trustworthy AI systems, which include Page 2 NIST AI 100-1 AI RMF 1.0 valid and reliable, safe, secure and resilient, accountable and transparent, explainable and\\ninterpretable, privacy enhanced, and fair with their harmful biases managed. Part 2 comprises the “Core” of the Framework. It describes four specific functions to help\\norganizations address the risks of AI systems in practice. These functions – GOVERN,\\nMAP, MEASURE, and MANAGE – are broken down further into categories and subcate-\\ngories. While GOVERN applies to all stages of organizations’ AI risk management pro-\\ncesses and procedures, the MAP, MEASURE, and MANAGE functions can be applied in AI\\nsystem-specific contexts and at specific stages of the AI lifecycle. Additional resources related to the Framework are included in the AI RMF Playbook,\\nwhich is available via the NIST AI RMF website:\\nhttps://www.nist.gov/itl/ai-risk-management-framework. Development of the AI RMF by NIST in collaboration with the private and public sec-\\ntors is directed and consistent with its broader AI efforts called for by the National AI\\nInitiative Act of 2020, the National Security Commission on Artificial Intelligence recom-\\nmendations, and the Plan for Federal Engagement in Developing Technical Standards and\\nRelated Tools. Engagement with the AI community during this Framework’s development\\n– via responses to a formal Request for Information, three widely attended workshops,\\npublic comments on a concept paper and two drafts of the Framework, discussions at mul-\\ntiple public forums, and many small group meetings – has informed development of the AI\\nRMF 1.0 as well as AI research and development and evaluation conducted by NIST and\\nothers. Priority research and additional guidance that will enhance this Framework will be\\ncaptured in an associated AI Risk Management Framework Roadmap to which NIST and\\nthe broader community can contribute. Page 3 NIST AI 100-1 AI RMF 1.0 Part 1: Foundational Information', 'Part 1: Foundational Information\\n', '1. Framing Risk\\nAI risk management offers a path to minimize potential negative impacts of AI systems,\\nsuch as threats to civil liberties and rights, while also providing opportunities to maximize\\npositive impacts. Addressing, documenting, and managing AI risks and potential negative\\nimpacts effectively can lead to more trustworthy AI systems.', '1.1 Understanding and Addressing Risks, Impacts, and Harms\\n1.2 Challenges for AI Risk Management In the context of the AI RMF, risk refers to the composite measure of an event’s probability\\nof occurring and the magnitude or degree of the consequences of the corresponding event.\\nThe impacts, or consequences, of AI systems can be positive, negative, or both and can\\nresult in opportunities or threats (Adapted from: ISO 31000:2018). When considering the\\nnegative impact of a potential event, risk is a function of 1) the negative impact, or magni-\\ntude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of\\noccurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be\\nexperienced by individuals, groups, communities, organizations, society, the environment,\\nand the planet. “Risk management refers to coordinated activities to direct and control an organiza-\\ntion with regard to risk” (Source: ISO 31000:2018). While risk management processes generally address negative impacts, this Framework of-\\nfers approaches to minimize anticipated negative impacts of AI systems and identify op-\\nportunities to maximize positive impacts. Effectively managing the risk of potential harms\\ncould lead to more trustworthy AI systems and unleash potential benefits to people (individ-\\nuals, communities, and society), organizations, and systems/ecosystems. Risk management\\ncan enable AI developers and users to understand impacts and account for the inherent lim-\\nitations and uncertainties in their models and systems, which in turn can improve overall\\nsystem performance and trustworthiness and the likelihood that AI technologies will be\\nused in ways that are beneficial. The AI RMF is designed to address new risks as they emerge. This flexibility is particularly\\nimportant where impacts are not easily foreseeable and applications are evolving. While\\nsome AI risks and benefits are well-known, it can be challenging to assess negative impacts\\nand the degree of harms. Figure 1 provides examples of potential harms that can be related\\nto AI systems. AI risk management efforts should consider that humans may assume that AI systems work\\n– and work well – in all settings. For example, whether correct or not, AI systems are\\noften perceived as being more objective than humans or as offering greater capabilities\\nthan general software. Page 4 NIST AI 100-1 AI RMF 1.0', '1.2 Challenges for AI Risk Management\\nSeveral challenges are described below. They should be taken into account when managing\\nrisks in pursuit of AI trustworthiness.', '1.2.1 Risk Measurement\\n1.2.2 Risk Tolerance\\n1.2.3 Risk Prioritization\\n1.2.4 Organizational Integration and Management of Risk AI risks or failures that are not well-defined or adequately understood are difficult to mea-\\nsure quantitatively or qualitatively. The inability to appropriately measure AI risks does not\\nimply that an AI system necessarily poses either a high or low risk. Some risk measurement\\nchallenges include: Risks related to third-party software, hardware, and data: Third-party data or systems\\ncan accelerate research and development and facilitate technology transition. They also\\nmay complicate risk measurement. Risk can emerge both from third-party data, software or\\nhardware itself and how it is used. Risk metrics or methodologies used by the organization\\ndeveloping the AI system may not align with the risk metrics or methodologies uses by\\nthe organization deploying or operating the system. Also, the organization developing\\nthe AI system may not be transparent about the risk metrics or methodologies it used. Risk\\nmeasurement and management can be complicated by how customers use or integrate third-\\nparty data or systems into AI products or services, particularly without sufficient internal\\ngovernance structures and technical safeguards. Regardless, all parties and AI actors should\\nmanage risk in the AI systems they develop, deploy, or use as standalone or integrated\\ncomponents. Tracking emergent risks: Organizations’ risk management efforts will be enhanced by\\nidentifying and tracking emergent risks and considering techniques for measuring them. Page 5 NIST AI 100-1 AI RMF 1.0 AI system impact assessment approaches can help AI actors understand potential impacts\\nor harms within specific contexts. Availability of reliable metrics: The current lack of consensus on robust and verifiable\\nmeasurement methods for risk and trustworthiness, and applicability to different AI use\\ncases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure\\nnegative risk or harms include the reality that development of metrics is often an institu-\\ntional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In\\naddition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-\\ncome relied upon in unexpected ways, or fail to account for differences in affected groups\\nand contexts. Approaches for measuring impacts on a population work best if they recognize that contexts\\nmatter, that harms may affect varied groups or sub-groups differently, and that communities\\nor other sub-groups who may be harmed are not always direct users of a system. Risk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI\\nlifecycle may yield different results than measuring risk at a later stage; some risks may\\nbe latent at a given point in time and may increase as AI systems adapt and evolve. Fur-\\nthermore, different AI actors across the AI lifecycle can have different risk perspectives.\\nFor example, an AI developer who makes AI software available, such as pre-trained mod-\\nels, can have a different risk perspective than an AI actor who is responsible for deploying\\nthat pre-trained model in a specific use case. Such deployers may not recognize that their\\nparticular uses could entail risks which differ from those perceived by the initial developer.\\nAll involved AI actors share responsibilities for designing, developing, and deploying a\\ntrustworthy AI system that is fit for purpose. Risk in real-world settings: While measuring AI risks in a laboratory or a controlled\\nenvironment may yield important insights pre-deployment, these measurements may differ\\nfrom risks that emerge in operational, real-world settings. Inscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability\\ncan be a result of the opaque nature of AI systems (limited explainability or interpretabil-\\nity), lack of transparency or documentation in AI system development or deployment, or\\ninherent uncertainties in AI systems. Human baseline: Risk management of AI systems that are intended to augment or replace\\nhuman activity, for example decision making, requires some form of baseline metrics for\\ncomparison. This is difficult to systematize since AI systems carry out different tasks – and\\nperform tasks differently – than humans. Page 6 NIST AI 100-1', '1.2.2 Risk Tolerance\\nAI RMF 1.0 While the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk\\ntolerance refers to the organization’s or AI actor’s (see Appendix A) readiness to bear the\\nrisk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-\\ntory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that\\nis acceptable to organizations or society are highly contextual and application and use-case\\nspecific. Risk tolerances can be influenced by policies and norms established by AI sys-\\ntem owners, organizations, industries, communities, or policy makers. Risk tolerances are\\nlikely to change over time as AI systems, policies, and norms evolve. Different organiza-\\ntions may have varied risk tolerances due to their particular organizational priorities and\\nresource considerations. Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-\\ntinue to be developed and debated by businesses, governments, academia, and civil society.\\nTo the extent that challenges for specifying AI risk tolerances remain unresolved, there may\\nbe contexts where a risk management framework is not yet readily applicable for mitigating\\nnegative AI risks. The Framework is intended to be flexible and to augment existing risk practices\\nwhich should align with applicable laws, regulations, and norms. Organizations\\nshould follow existing regulations and guidelines for risk criteria, tolerance, and\\nresponse established by organizational, domain, discipline, sector, or professional\\nrequirements. Some sectors or industries may have established definitions of harm or\\nestablished documentation, reporting, and disclosure requirements. Within sectors,\\nrisk management may depend on existing guidelines for specific applications and\\nuse case settings. Where established guidelines do not exist, organizations should\\ndefine reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used\\nto manage risks and to document risk management processes.', '1.2.3 Risk Prioritization\\nAttempting to eliminate negative risk entirely can be counterproductive in practice because\\nnot all incidents and failures can be eliminated. Unrealistic expectations about risk may\\nlead organizations to allocate resources in a manner that makes risk triage inefficient or\\nimpractical or wastes scarce resources. A risk management culture can help organizations\\nrecognize that not all AI risks are the same, and resources can be allocated purposefully.\\nActionable risk management efforts lay out clear guidelines for assessing trustworthiness\\nof each AI system an organization develops or deploys. Policies and resources should be\\nprioritized based on the assessed risk level and potential impact of an AI system. The extent\\nto which an AI system may be customized or tailored to the specific context of use by the\\nAI deployer can be a contributing factor. Page 7 NIST AI 100-1 AI RMF 1.0 When applying the AI RMF, risks which the organization determines to be highest for the\\nAI systems within a given context of use call for the most urgent prioritization and most\\nthorough risk management process. In cases where an AI system presents unacceptable\\nnegative risk levels – such as where significant negative impacts are imminent, severe harms\\nare actually occurring, or catastrophic risks are present – development and deployment\\nshould cease in a safe manner until risks can be sufficiently managed. If an AI system’s\\ndevelopment, deployment, and use cases are found to be low-risk in a specific context, that\\nmay suggest potentially lower prioritization. Risk prioritization may differ between AI systems that are designed or deployed to directly\\ninteract with humans as compared to AI systems that are not. Higher initial prioritization\\nmay be called for in settings where the AI system is trained on large datasets comprised of\\nsensitive or protected data such as personally identifiable information, or where the outputs\\nof the AI systems have direct or indirect impact on humans. AI systems designed to interact\\nonly with computational systems and trained on non-sensitive datasets (for example, data\\ncollected from the physical environment) may call for lower initial prioritization. Nonethe-\\nless, regularly assessing and prioritizing risk based on context remains important because\\nnon-human-facing AI systems can have downstream safety or social implications. Residual risk – defined as risk remaining after risk treatment (Source: ISO GUIDE 73) –\\ndirectly impacts end users or affected individuals and communities. Documenting residual\\nrisks will call for the system provider to fully consider the risks of deploying the AI product\\nand will inform end users about potential negative impacts of interacting with the system.', '1.2.4 Organizational Integration and Management of Risk\\nAI risks should not be considered in isolation. Different AI actors have different responsi-\\nbilities and awareness depending on their roles in the lifecycle. For example, organizations\\ndeveloping an AI system often will not have information about how the system may be\\nused. AI risk management should be integrated and incorporated into broader enterprise\\nrisk management strategies and processes. Treating AI risks along with other critical risks,\\nsuch as cybersecurity and privacy, will yield a more integrated outcome and organizational\\nefficiencies. The AI RMF may be utilized along with related guidance and frameworks for managing\\nAI system risks or broader enterprise risks. Some risks related to AI systems are common\\nacross other types of software development and deployment. Examples of overlapping risks\\ninclude: privacy concerns related to the use of underlying data to train AI systems; the en-\\nergy and environmental implications associated with resource-heavy computing demands;\\nsecurity concerns related to the confidentiality, integrity, and availability of the system and\\nits training and output data; and general security of the underlying software and hardware\\nfor AI systems. Page 8 NIST AI 100-1 AI RMF 1.0 Organizations need to establish and maintain the appropriate accountability mechanisms,\\nroles and responsibilities, culture, and incentive structures for risk management to be ef-\\nfective. Use of the AI RMF alone will not lead to these changes or provide the appropriate\\nincentives. Effective risk management is realized through organizational commitment at\\nsenior levels and may require cultural change within an organization or industry. In addi-\\ntion, small to medium-sized organizations managing AI risks or implementing the AI RMF\\nmay face different challenges than large organizations, depending on their capabilities and\\nresources.', '2. Audience\\n3 AI Risks and Trustworthiness Identifying and managing AI risks and potential impacts – both positive and negative – re-\\nquires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will\\nrepresent a diversity of experience, expertise, and backgrounds and comprise demograph-\\nically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors\\nacross the AI lifecycle and dimensions. The OECD has developed a framework for classifying AI lifecycle activities according to\\nfive key socio-technical dimensions, each with properties relevant for AI policy and gover-\\nnance, including risk management [OECD (2022) OECD Framework for the Classification\\nof AI systems — OECD Digital Economy Papers]. Figure 2 shows these dimensions,\\nslightly modified by NIST for purposes of this framework. The NIST modification high-\\nlights the importance of test, evaluation, verification, and validation (TEVV) processes\\nthroughout an AI lifecycle and generalizes the operational context of an AI system. AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI\\nModel, and Task and Output. AI actors involved in these dimensions who perform or\\nmanage the design, development, deployment, evaluation, and use of AI systems and drive\\nAI risk management efforts are the primary AI RMF audience. Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described\\nin detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks\\nand achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific\\nexpertise are integrated throughout the AI lifecycle and are especially likely to benefit from\\nthe Framework. Performed regularly, TEVV tasks can provide insights relative to technical,\\nsocietal, legal, and ethical standards or norms, and can assist with anticipating impacts and\\nassessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV\\nallows for both mid-course remediation and post-hoc risk management. The People & Planet dimension at the center of Figure 2 represents human rights and the\\nbroader well-being of society and the planet. The AI actors in this dimension comprise\\na separate AI RMF audience who informs the primary audience. These AI actors may in-\\nclude trade associations, standards developing organizations, researchers, advocacy groups, Page 9 NIST AI 100-1 AI RMF 1.0 environmental groups, civil society organizations, end users, and potentially impacted in-\\ndividuals and communities. These actors can: • assist in providing context and understanding potential and actual impacts;\\n• be a source of formal or quasi-formal norms and guidance for AI risk management;\\n• designate boundaries for AI operation (technical, societal, legal, and ethical); and\\n• promote discussion of the tradeoffs needed to balance societal values and priorities\\nrelated to civil liberties and rights, equity, the environment and the planet, and the\\neconomy. Successful risk management depends upon a sense of collective responsibility among AI\\nactors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse\\nperspectives, disciplines, professions, and experiences. Diverse teams contribute to more\\nopen sharing of ideas and assumptions about the purposes and functions of technology –\\nmaking these implicit aspects more explicit. This broader collective perspective creates\\nopportunities for surfacing problems and identifying existing and emergent risks. Page 10 NIST AI 100-1 AI RMF 1.0 h\\nt\\ni w ,\\ne\\nc\\ni\\nt\\nc\\na\\nr\\np\\nt\\ns\\ne\\nb\\na s\\na\\nd\\ne\\nt\\na\\nr\\na\\np\\ne\\ns e\\nr\\na )\\n2\\ne\\nr\\nu\\ng\\ni\\nF\\n( n\\no\\ni\\ns\\nn\\ne\\nm\\ni\\nd l\\ne\\nd\\no\\nM I A\\ne\\nh\\nt n\\ni s\\nr\\no\\nt\\nc\\na I A t\\na\\nh\\nt e\\nt\\no\\nN .\\ns\\nk\\ns\\na\\nt n\\no\\ni\\nt\\na\\nd\\ni\\nl\\na\\nv\\nd\\nn\\na , n\\no\\ni\\nt\\na\\nc\\nfi i\\nr\\ne\\nv , n\\no\\ni\\nt\\na\\nu\\nl\\na\\nv\\ne , g\\nn\\ni\\nt\\ns\\ne\\nt t\\nu\\no\\nb\\na s\\nl\\ni\\na\\nt\\ne\\nd g\\nn\\ni\\nd\\nu\\nl\\nc\\nn\\ni ,\\ns\\nk\\ns\\na\\nt r\\no\\nt\\nc\\na I A f\\no s\\nn\\no\\ni\\nt\\np\\ni\\nr\\nc\\ns\\ne\\nd d\\ne\\nl\\ni\\na\\nt\\ne\\nd r\\no\\nf A\\nx\\ni\\nd\\nn\\ne\\np\\np\\nA\\ne\\ne\\nS .\\ns\\ne\\ng\\na\\nt\\ns e\\nl\\nc\\ny\\nc\\ne\\nf\\ni\\nl I A\\ns\\ns\\no\\nr\\nc\\na s\\nr\\no\\nt\\nc\\na I A . 3 .\\ng\\ni\\nF .\\ns\\nl\\ne\\nd\\no\\nm\\ne\\nh\\nt g\\nn\\ni\\nt\\na\\nd\\ni\\nl\\na\\nv d\\nn\\na g\\nn\\ni\\ny\\nf\\ni\\nr\\ne\\nv e\\ns\\no\\nh\\nt m\\no\\nr\\nf d\\ne\\nt\\na\\nr\\na\\np\\ne\\ns s\\nl\\ne\\nd\\no\\nm\\ne\\nh\\nt g\\nn\\ni\\ns\\nu\\nd\\nn\\na\\ng\\nn\\ni\\nd\\nl\\ni\\nu\\nb e\\ns\\no\\nh\\nt Page 11 NIST AI 100-1 AI RMF 1.0', '3. AI Risks and Trustworthiness\\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-\\nteria that are of value to interested parties. Approaches which enhance AI trustworthiness\\ncan reduce negative AI risks. This Framework articulates the following characteristics of\\ntrustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI\\nsystems include: valid and reliable, safe, secure and resilient, accountable and trans-\\nparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias\\nmanaged. Creating trustworthy AI requires balancing each of these characteristics based\\non the AI system’s context of use. While all characteristics are socio-technical system at-\\ntributes, accountability and transparency also relate to the processes and activities internal\\nto an AI system and its external setting. Neglecting these characteristics can increase the\\nprobability and magnitude of negative consequences. Trustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-\\nnizational behavior, the datasets used by AI systems, selection of AI models and algorithms\\nand the decisions made by those who build them, and the interactions with the humans who\\nprovide insight from and oversight of such systems. Human judgment should be employed\\nwhen deciding on the specific metrics related to AI trustworthiness characteristics and the\\nprecise threshold values for those metrics. Addressing AI trustworthiness characteristics individually will not ensure AI system trust-\\nworthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-\\nting, and some will be more or less important in any given situation. Ultimately, trustwor-\\nthiness is a social concept that ranges across a spectrum and is only as strong as its weakest\\ncharacteristics. When managing AI risks, organizations can face difficult decisions in balancing these char-\\nacteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for\\ninterpretability and achieving privacy. In other cases, organizations might face a tradeoff\\nbetween predictive accuracy and interpretability. Or, under certain conditions such as data\\nsparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions Page 12 NIST AI 100-1 AI RMF 1.0 about fairness and other values in certain domains. Dealing with tradeoffs requires tak-\\ning into account the decision-making context. These analyses can highlight the existence\\nand extent of tradeoffs between different measures, but they do not answer questions about\\nhow to navigate the tradeoff. Those depend on the values at play in the relevant context and\\nshould be resolved in a manner that is both transparent and appropriately justifiable. There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For\\nexample, subject matter experts can assist in the evaluation of TEVV findings and work\\nwith product and deployment teams to align TEVV parameters to requirements and de-\\nployment conditions. When properly resourced, increasing the breadth and diversity of\\ninput from interested parties and relevant AI actors throughout the AI lifecycle can en-\\nhance opportunities for informing contextually sensitive evaluations, and for identifying\\nAI system benefits and positive impacts. These practices can increase the likelihood that\\nrisks arising in social contexts are managed appropriately. Understanding and treatment of trustworthiness characteristics depends on an AI actor’s\\nparticular role within the AI lifecycle. For any given AI system, an AI designer or developer\\nmay have a different perception of the characteristics than the deployer. Trustworthiness characteristics explained in this document influence each other.\\nHighly secure but unfair systems, accurate but opaque and uninterpretable systems,\\nand inaccurate but secure, privacy-enhanced, and transparent systems are all unde-\\nsirable. A comprehensive approach to risk management calls for balancing tradeoffs\\namong the trustworthiness characteristics. It is the joint responsibility of all AI ac-\\ntors to determine whether AI technology is an appropriate or necessary tool for a\\ngiven context or purpose, and how to use it responsibly. The decision to commission\\nor deploy an AI system should be based on a contextual assessment of trustworthi-\\nness characteristics and the relative risks, impacts, costs, and benefits, and informed\\nby a broad set of interested parties.', '3.1 Valid and Reliable\\n3.2 Safe\\n3.3 Secure and Resilient\\n3.4 Accountable and Transparent\\n3.5 Explainable and Interpretable\\n3.6 Privacy-Enhanced\\n3.7 Fair – with Harmful Bias Managed Validation is the “confirmation, through the provision of objective evidence, that the re-\\nquirements for a specific intended use or application have been fulfilled” (Source:\\nISO\\n9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-\\nalized to data and settings beyond their training creates and increases negative AI risks and\\nreduces trustworthiness. Reliability is defined in the same standard as the “ability of an item to perform as required,\\nwithout failure, for a given time interval, under given conditions” (Source:\\nISO/IEC TS\\n5723:2022). Reliability is a goal for overall correctness of AI system operation under the\\nconditions of expected use and over a given period of time, including the entire lifetime of\\nthe system. Page 13 NIST AI 100-1 AI RMF 1.0 Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and\\ncan be in tension with one another in AI systems. Accuracy is defined by ISO/IEC TS 5723:2022 as “closeness of results of observations,\\ncomputations, or estimates to the true values or the values accepted as being true.” Mea-\\nsures of accuracy should consider computational-centric measures (e.g., false positive and\\nfalse negative rates), human-AI teaming, and demonstrate external validity (generalizable\\nbeyond the training conditions). Accuracy measurements should always be paired with\\nclearly defined and realistic test sets – that are representative of conditions of expected use\\n– and details about test methodology; these should be included in associated documen-\\ntation. Accuracy measurements may include disaggregation of results for different data\\nsegments. Robustness or generalizability is defined as the “ability of a system to maintain its level\\nof performance under a variety of circumstances” (Source: ISO/IEC TS 5723:2022). Ro-\\nbustness is a goal for appropriate system functionality in a broad set of conditions and\\ncircumstances, including uses of AI systems not initially anticipated. Robustness requires\\nnot only that the system perform exactly as it does under expected uses, but also that it\\nshould perform in ways that minimize potential harms to people if it is operating in an\\nunexpected setting. Validity and reliability for deployed AI systems are often assessed by ongoing testing or\\nmonitoring that confirms a system is performing as intended. Measurement of validity,\\naccuracy, robustness, and reliability contribute to trustworthiness and should take into con-\\nsideration that certain types of failures can cause greater harm. AI risk management efforts\\nshould prioritize the minimization of potential negative impacts, and may need to include\\nhuman intervention in cases where the AI system cannot detect or correct errors.', '3.2 Safe\\nAI systems should “not under defined conditions, lead to a state in which human life,\\nhealth, property, or the environment is endangered” (Source: ISO/IEC TS 5723:2022). Safe\\noperation of AI systems is improved through: • responsible design, development, and deployment practices;\\n• clear information to deployers on responsible use of the system;\\n• responsible decision-making by deployers and end users; and\\n• explanations and documentation of risks based on empirical evidence of incidents. Different types of safety risks may require tailored AI risk management approaches based\\non context and the severity of potential risks presented. Safety risks that pose a potential\\nrisk of serious injury or death call for the most urgent prioritization and most thorough risk\\nmanagement process. Page 14 NIST AI 100-1 AI RMF 1.0 Employing safety considerations during the lifecycle and starting as early as possible with\\nplanning and design can prevent failures or conditions that can render a system dangerous.\\nOther practical approaches for AI safety often relate to rigorous simulation and in-domain\\ntesting, real-time monitoring, and the ability to shut down, modify, or have human inter-\\nvention into systems that deviate from intended or expected functionality. AI safety risk management approaches should take cues from efforts and guidelines for\\nsafety in fields such as transportation and healthcare, and align with existing sector- or\\napplication-specific guidelines or standards.', '3.3 Secure and Resilient\\nAI systems, as well as the ecosystems in which they are deployed, may be said to be re-\\nsilient if they can withstand unexpected adverse events or unexpected changes in their envi-\\nronment or use – or if they can maintain their functions and structure in the face of internal\\nand external change and degrade safely and gracefully when this is necessary (Adapted\\nfrom: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples,\\ndata poisoning, and the exfiltration of models, training data, or other intellectual property\\nthrough AI system endpoints. AI systems that can maintain confidentiality, integrity, and\\navailability through protection mechanisms that prevent unauthorized access and use may\\nbe said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage-\\nment Framework are among those which are applicable here. Security and resilience are related but distinct characteristics. While resilience is the abil-\\nity to return to normal function after an unexpected adverse event, security includes re-\\nsilience but also encompasses protocols to avoid, protect against, respond to, or recover\\nfrom attacks. Resilience relates to robustness and goes beyond the provenance of the data\\nto encompass unexpected or adversarial use (or abuse or misuse) of the model or data.', '3.4 Accountable and Transparent\\nTrustworthy AI depends upon accountability. Accountability presupposes transparency.\\nTransparency reflects the extent to which information about an AI system and its outputs is\\navailable to individuals interacting with such a system – regardless of whether they are even\\naware that they are doing so. Meaningful transparency provides access to appropriate levels\\nof information based on the stage of the AI lifecycle and tailored to the role or knowledge\\nof AI actors or individuals interacting with or using the AI system. By promoting higher\\nlevels of understanding, transparency increases confidence in the AI system. This characteristic’s scope spans from design decisions and training data to model train-\\ning, the structure of the model, its intended use cases, and how and when deployment,\\npost-deployment, or end user decisions were made and by whom. Transparency is often\\nnecessary for actionable redress related to AI system outputs that are incorrect or otherwise\\nlead to negative impacts. Transparency should consider human-AI interaction: for exam- Page 15 NIST AI 100-1 AI RMF 1.0 ple, how a human operator or user is notified when a potential or actual adverse outcome\\ncaused by an AI system is detected. A transparent system is not necessarily an accurate,\\nprivacy-enhanced, secure, or fair system. However, it is difficult to determine whether an\\nopaque system possesses such characteristics, and to do so over time as complex systems\\nevolve. The role of AI actors should be considered when seeking accountability for the outcomes of\\nAI systems. The relationship between risk and accountability associated with AI and tech-\\nnological systems more broadly differs across cultural, legal, sectoral, and societal contexts.\\nWhen consequences are severe, such as when life and liberty are at stake, AI developers\\nand deployers should consider proportionally and proactively adjusting their transparency\\nand accountability practices. Maintaining organizational practices and governing structures\\nfor harm reduction, like risk management, can help lead to more accountable systems. Measures to enhance transparency and accountability should also consider the impact of\\nthese efforts on the implementing entity, including the level of necessary resources and the\\nneed to safeguard proprietary information. Maintaining the provenance of training data and supporting attribution of the AI system’s\\ndecisions to subsets of training data can assist with both transparency and accountability.\\nTraining data may also be subject to copyright and should follow applicable intellectual\\nproperty rights laws. As transparency tools for AI systems and related documentation continue to evolve, devel-\\nopers of AI systems are encouraged to test different types of transparency tools in cooper-\\nation with AI deployers to ensure that AI systems are used as intended.', '3.5 Explainable and Interpretable\\nExplainability refers to a representation of the mechanisms underlying AI systems’ oper-\\nation, whereas interpretability refers to the meaning of AI systems’ output in the context\\nof their designed functional purposes. Together, explainability and interpretability assist\\nthose operating or overseeing an AI system, as well as users of an AI system, to gain\\ndeeper insights into the functionality and trustworthiness of the system, including its out-\\nputs. The underlying assumption is that perceptions of negative risk stem from a lack of\\nability to make sense of, or contextualize, system output appropriately. Explainable and\\ninterpretable AI systems offer information that will help end users understand the purposes\\nand potential impact of an AI system. Risk from lack of explainability may be managed by describing how AI systems function,\\nwith descriptions tailored to individual differences such as the user’s role, knowledge, and\\nskill level. Explainable systems can be debugged and monitored more easily, and they lend\\nthemselves to more thorough documentation, audit, and governance. Page 16 NIST AI 100-1 AI RMF 1.0 Risks to interpretability often can be addressed by communicating a description of why\\nan AI system made a particular prediction or recommendation. (See “Four Principles of\\nExplainable Artificial Intelligence” and “Psychological Foundations of Explainability and\\nInterpretability in Artificial Intelligence” found here.) Transparency, explainability, and interpretability are distinct characteristics that support\\neach other. Transparency can answer the question of “what happened” in the system. Ex-\\nplainability can answer the question of “how” a decision was made in the system. Inter-\\npretability can answer the question of “why” a decision was made by the system and its\\nmeaning or context to the user.', '3.6 Privacy-Enhanced\\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy,\\nidentity, and dignity. These norms and practices typically address freedom from intrusion,\\nlimiting observation, or individuals’ agency to consent to disclosure or control of facets of\\ntheir identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool\\nfor Improving Privacy through Enterprise Risk Management.) Privacy values such as anonymity, confidentiality, and control generally should guide choices\\nfor AI system design, development, and deployment. Privacy-related risks may influence\\nsecurity, bias, and transparency and come with tradeoffs with these other characteristics.\\nLike safety and security, specific technical features of an AI system may promote or reduce\\nprivacy. AI systems can also present new risks to privacy by allowing inference to identify\\nindividuals or previously private information about individuals. Privacy-enhancing technologies (“PETs”) for AI, as well as data minimizing methods such\\nas de-identification and aggregation for certain model outputs, can support design for\\nprivacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-\\nenhancing techniques can result in a loss in accuracy, affecting decisions about fairness\\nand other values in certain domains.', 'Fair – with Harmful Bias Managed\\n', '4. Effectiveness of the AI RMF\\nPart 2: Core and Profiles\\n5 AI RMF Core Evaluations of AI RMF effectiveness – including ways to measure bottom-line improve-\\nments in the trustworthiness of AI systems – will be part of future NIST activities, in\\nconjunction with the AI community. Organizations and other users of the Framework are encouraged to periodically evaluate\\nwhether the AI RMF has improved their ability to manage AI risks, including but not lim-\\nited to their policies, processes, practices, implementation plans, indicators, measurements,\\nand expected outcomes. NIST intends to work collaboratively with others to develop met-\\nrics, methodologies, and goals for evaluating the AI RMF’s effectiveness, and to broadly\\nshare results and supporting information. Framework users are expected to benefit from: • enhanced processes for governing, mapping, measuring, and managing AI risk, and clearly documenting outcomes; • improved awareness of the relationships and tradeoffs among trustworthiness char- acteristics, socio-technical approaches, and AI risks; • explicit processes for making go/no-go system commissioning and deployment deci- sions; • established policies, processes, practices, and procedures for improving organiza- tional accountability efforts related to AI system risks; • enhanced organizational culture which prioritizes the identification and management\\nof AI system risks and potential impacts to individuals, communities, organizations,\\nand society; • better information sharing within and across organizations about risks, decision-\\nmaking processes, responsibilities, common pitfalls, TEVV practices, and approaches\\nfor continuous improvement; • greater contextual knowledge for increased awareness of downstream risks;\\n• strengthened engagement with interested parties and relevant AI actors; and\\n• augmented capacity for TEVV of AI systems and associated risks. Page 19 NIST AI 100-1 Part 2: Core and Profiles', 'Part 2: Core and Profiles\\n', '5. AI RMF Core\\nAI RMF 1.0 The AI RMF Core provides outcomes and actions that enable dialogue, understanding, and\\nactivities to manage AI risks and responsibly develop trustworthy AI systems. As illus-\\ntrated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE,\\nand MANAGE. Each of these high-level functions is broken down into categories and sub-\\ncategories. Categories and subcategories are subdivided into specific actions and outcomes.\\nActions do not constitute a checklist, nor are they necessarily an ordered set of steps. Risk management should be continuous, timely, and performed throughout the AI system\\nlifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects\\ndiverse and multidisciplinary perspectives, potentially including the views of AI actors out-\\nside the organization. Having a diverse team contributes to more open sharing of ideas and\\nassumptions about purposes and functions of the technology being designed, developed, Page 20 NIST AI 100-1 AI RMF 1.0 deployed, or evaluated – which can create opportunities to surface problems and identify\\nexisting and emergent risks. An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available\\nto help organizations navigate the AI RMF and achieve its outcomes through suggested\\ntactical actions they can apply within their own contexts. Like the AI RMF, the Playbook\\nis voluntary and organizations can utilize the suggestions according to their needs and\\ninterests. Playbook users can create tailored guidance selected from suggested material\\nfor their own use and contribute their suggestions for sharing with the broader community.\\nAlong with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI\\nResource Center. Framework users may apply these functions as best suits their needs for managing\\nAI risks based on their resources and capabilities. Some organizations may choose\\nto select from among the categories and subcategories; others may choose and have\\nthe capacity to apply all categories and subcategories. Assuming a governance struc-\\nture is in place, functions may be performed in any order across the AI lifecycle as\\ndeemed to add value by a user of the framework. After instituting the outcomes in\\nGOVERN, most users of the AI RMF would start with the MAP function and con-\\ntinue to MEASURE or MANAGE. However users integrate the functions, the process\\nshould be iterative, with cross-referencing between functions as necessary. Simi-\\nlarly, there are categories and subcategories with elements that apply to multiple\\nfunctions, or that logically should take place before certain subcategory decisions.', '5.1 Govern\\n5.2 Map\\n5.3 Measure\\n5.4 Manage The GOVERN function: • cultivates and implements a culture of risk management within organizations design- ing, developing, deploying, evaluating, or acquiring AI systems; • outlines processes, documents, and organizational schemes that anticipate, identify,\\nand manage the risks a system can pose, including to users and others across society\\n– and procedures to achieve those outcomes; • incorporates processes to assess potential impacts;\\n• provides a structure by which AI risk management functions can align with organi- zational principles, policies, and strategic priorities; • connects technical aspects of AI system design and development to organizational\\nvalues and principles, and enables organizational practices and competencies for the\\nindividuals involved in acquiring, training, deploying, and monitoring such systems;\\nand • addresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data. Page 21 NIST AI 100-1 AI RMF 1.0 GOVERN is a cross-cutting function that is infused throughout AI risk management and\\nenables the other functions of the process. Aspects of GOVERN, especially those related to\\ncompliance or evaluation, should be integrated into each of the other functions. Attention\\nto governance is a continual and intrinsic requirement for effective AI risk management\\nover an AI system’s lifespan and the organization’s hierarchy. Strong governance can drive and enhance internal practices and norms to facilitate orga-\\nnizational risk culture. Governing authorities can determine the overarching policies that\\ndirect an organization’s mission, goals, values, culture, and risk tolerance. Senior leader-\\nship sets the tone for risk management within an organization, and with it, organizational\\nculture. Management aligns the technical aspects of AI risk management to policies and\\noperations. Documentation can enhance transparency, improve human review processes,\\nand bolster accountability in AI system teams. After putting in place the structures, systems, processes, and teams described in the GOV-\\nERN function, organizations should benefit from a purpose-driven culture focused on risk\\nunderstanding and management. It is incumbent on Framework users to continue to ex-\\necute the GOVERN function as knowledge, cultures, and needs or expectations from AI\\nactors evolve over time. Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table\\n1 lists the GOVERN function’s categories and subcategories. Table 1: Categories and subcategories for the GOVERN function. Categories GOVERN 1:\\nPolicies, processes,\\nprocedures, and\\npractices across the\\norganization related\\nto the mapping,\\nmeasuring, and\\nmanaging of AI\\nrisks are in place,\\ntransparent, and\\nimplemented\\neffectively. Subcategories\\nGOVERN 1.1: Legal and regulatory requirements involving AI\\nare understood, managed, and documented.\\nGOVERN 1.2: The characteristics of trustworthy AI are inte-\\ngrated into organizational policies, processes, procedures, and\\npractices.\\nGOVERN 1.3: Processes, procedures, and practices are in place\\nto determine the needed level of risk management activities based\\non the organization’s risk tolerance.\\nGOVERN 1.4: The risk management process and its outcomes are\\nestablished through transparent policies, procedures, and other\\ncontrols based on organizational risk priorities. Continued on next page Page 22 NIST AI 100-1 AI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued)\\nCategories GOVERN 2:\\nAccountability\\nstructures are in\\nplace so that the\\nappropriate teams\\nand individuals are\\nempowered,\\nresponsible, and\\ntrained for mapping,\\nmeasuring, and\\nmanaging AI risks. GOVERN 3:\\nWorkforce diversity,\\nequity, inclusion,\\nand accessibility\\nprocesses are\\nprioritized in the\\nmapping,\\nmeasuring, and\\nmanaging of AI\\nrisks throughout the\\nlifecycle.\\nGOVERN 4:\\nOrganizational\\nteams are committed\\nto a culture Subcategories\\nGOVERN 1.5: Ongoing monitoring and periodic review of the\\nrisk management process and its outcomes are planned and or-\\nganizational roles and responsibilities clearly defined, including\\ndetermining the frequency of periodic review.\\nGOVERN 1.6: Mechanisms are in place to inventory AI systems\\nand are resourced according to organizational risk priorities.\\nGOVERN 1.7: Processes and procedures are in place for decom-\\nmissioning and phasing out AI systems safely and in a man-\\nner that does not increase risks or decrease the organization’s\\ntrustworthiness.\\nGOVERN 2.1: Roles and responsibilities and lines of communi-\\ncation related to mapping, measuring, and managing AI risks are\\ndocumented and are clear to individuals and teams throughout\\nthe organization.\\nGOVERN 2.2: The organization’s personnel and partners receive\\nAI risk management training to enable them to perform their du-\\nties and responsibilities consistent with related policies, proce-\\ndures, and agreements.\\nGOVERN 2.3: Executive leadership of the organization takes re-\\nsponsibility for decisions about risks associated with AI system\\ndevelopment and deployment.\\nGOVERN 3.1: Decision-making related to mapping, measuring,\\nand managing AI risks throughout the lifecycle is informed by a\\ndiverse team (e.g., diversity of demographics, disciplines, expe-\\nrience, expertise, and backgrounds). GOVERN 3.2: Policies and procedures are in place to define and\\ndifferentiate roles and responsibilities for human-AI configura-\\ntions and oversight of AI systems. GOVERN 4.1: Organizational policies and practices are in place\\nto foster a critical thinking and safety-first mindset in the design,\\ndevelopment, deployment, and uses of AI systems to minimize\\npotential negative impacts. Continued on next page Page 23 NIST AI 100-1 AI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued)\\nCategories Subcategories\\nGOVERN 4.2: Organizational teams document the risks and po-\\ntential impacts of the AI technology they design, develop, deploy,\\nevaluate, and use, and they communicate about the impacts more\\nbroadly.\\nGOVERN 4.3: Organizational practices are in place to enable AI\\ntesting, identification of incidents, and information sharing.\\nGOVERN 5.1: Organizational policies and practices are in place\\nto collect, consider, prioritize, and integrate feedback from those\\nexternal to the team that developed or deployed the AI system\\nregarding the potential individual and societal impacts related to\\nAI risks.\\nGOVERN 5.2: Mechanisms are established to enable the team\\nthat developed or deployed AI systems to regularly incorporate\\nadjudicated feedback from relevant AI actors into system design\\nand implementation.\\nGOVERN 6.1: Policies and procedures are in place that address\\nAI risks associated with third-party entities, including risks of in-\\nfringement of a third-party’s intellectual property or other rights.\\nGOVERN 6.2: Contingency processes are in place to handle\\nfailures or incidents in third-party data or AI systems deemed to\\nbe high-risk. that considers and\\ncommunicates AI\\nrisk. GOVERN 5:\\nProcesses are in\\nplace for robust\\nengagement with\\nrelevant AI actors. GOVERN 6: Policies\\nand procedures are\\nin place to address\\nAI risks and benefits\\narising from\\nthird-party software\\nand data and other\\nsupply chain issues.', '5.2 Map\\nThe MAP function establishes the context to frame risks related to an AI system. The AI\\nlifecycle consists of many interdependent activities involving a diverse set of actors (See\\nFigure 3). In practice, AI actors in charge of one part of the process often do not have full\\nvisibility or control over other parts and their associated contexts. The interdependencies\\nbetween these activities, and among the relevant AI actors, can make it difficult to reliably\\nanticipate impacts of AI systems. For example, early decisions in identifying purposes and\\nobjectives of an AI system can alter its behavior and capabilities, and the dynamics of de-\\nployment setting (such as end users or impacted individuals) can shape the impacts of AI\\nsystem decisions. As a result, the best intentions within one dimension of the AI lifecycle\\ncan be undermined via interactions with decisions and conditions in other, later activities. Page 24 NIST AI 100-1 AI RMF 1.0 This complexity and varying levels of visibility can introduce uncertainty into risk man-\\nagement practices. Anticipating, assessing, and otherwise addressing potential sources of\\nnegative risk can mitigate this uncertainty and enhance the integrity of the decision process. The information gathered while carrying out the MAP function enables negative risk pre-\\nvention and informs decisions for processes such as model management, as well as an\\ninitial decision about appropriateness or the need for an AI solution. Outcomes in the\\nMAP function are the basis for the MEASURE and MANAGE functions. Without contex-\\ntual knowledge, and awareness of risks within the identified contexts, risk management is\\ndifficult to perform. The MAP function is intended to enhance an organization’s ability to\\nidentify risks and broader contributing factors. Implementation of this function is enhanced by incorporating perspectives from a diverse\\ninternal team and engagement with those external to the team that developed or deployed\\nthe AI system. Engagement with external collaborators, end users, potentially impacted\\ncommunities, and others may vary based on the risk level of a particular AI system, the\\nmakeup of the internal team, and organizational policies. Gathering such broad perspec-\\ntives can help organizations proactively prevent negative risks and develop more trustwor-\\nthy AI systems by: • improving their capacity for understanding contexts;\\n• checking their assumptions about context of use;\\n• enabling recognition of when systems are not functional within or out of their in- tended context; • identifying positive and beneficial uses of their existing AI systems;\\n• improving understanding of limitations in AI and ML processes;\\n• identifying constraints in real-world applications that may lead to negative impacts;\\n• identifying known and foreseeable negative impacts related to intended use of AI systems; and • anticipating risks of the use of AI systems beyond intended use. After completing the MAP function, Framework users should have sufficient contextual\\nknowledge about AI system impacts to inform an initial go/no-go decision about whether\\nto design, develop, or deploy an AI system. If a decision is made to proceed, organizations\\nshould utilize the MEASURE and MANAGE functions along with policies and procedures\\nput into place in the GOVERN function to assist in AI risk management efforts. It is incum-\\nbent on Framework users to continue applying the MAP function to AI systems as context,\\ncapabilities, risks, benefits, and potential impacts evolve over time. Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table\\n2 lists the MAP function’s categories and subcategories. Page 25 NIST AI 100-1 AI RMF 1.0 Table 2: Categories and subcategories for the MAP function. Categories MAP 1: Context is\\nestablished and\\nunderstood. MAP 2:\\nCategorization of\\nthe AI system is\\nperformed. Subcategories\\nMAP 1.1: Intended purposes, potentially beneficial uses, context-\\nspecific laws, norms and expectations, and prospective settings in\\nwhich the AI system will be deployed are understood and docu-\\nmented. Considerations include: the specific set or types of users\\nalong with their expectations; potential positive and negative im-\\npacts of system uses to individuals, communities, organizations,\\nsociety, and the planet; assumptions and related limitations about\\nAI system purposes, uses, and risks across the development or\\nproduct AI lifecycle; and related TEVV and system metrics.\\nMAP 1.2: Interdisciplinary AI actors, competencies, skills, and\\ncapacities for establishing context reflect demographic diversity\\nand broad domain and user experience expertise, and their par-\\nticipation is documented. Opportunities for interdisciplinary col-\\nlaboration are prioritized.\\nMAP 1.3: The organization’s mission and relevant goals for AI\\ntechnology are understood and documented.\\nMAP 1.4: The business value or context of business use has been\\nclearly defined or – in the case of assessing existing AI systems\\n– re-evaluated.\\nMAP 1.5: Organizational risk tolerances are determined and\\ndocumented.\\nMAP 1.6: System requirements (e.g., “the system shall respect\\nthe privacy of its users”) are elicited from and understood by rel-\\nevant AI actors. Design decisions take socio-technical implica-\\ntions into account to address AI risks.\\nMAP 2.1: The specific tasks and methods used to implement the\\ntasks that the AI system will support are defined (e.g., classifiers,\\ngenerative models, recommenders).\\nMAP 2.2: Information about the AI system’s knowledge limits\\nand how system output may be utilized and overseen by humans\\nis documented. Documentation provides sufficient information\\nto assist relevant AI actors when making decisions and taking\\nsubsequent actions. Continued on next page Page 26 NIST AI 100-1 AI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued) Categories MAP 3: AI\\ncapabilities, targeted\\nusage, goals, and\\nexpected benefits\\nand costs compared\\nwith appropriate\\nbenchmarks are\\nunderstood. MAP 4: Risks and\\nbenefits are mapped\\nfor all components\\nof the AI system\\nincluding third-party\\nsoftware and data. MAP 5: Impacts to\\nindividuals, groups,\\ncommunities,\\norganizations, and\\nsociety are\\ncharacterized. Subcategories\\nMAP 2.3: Scientific integrity and TEVV considerations are iden-\\ntified and documented, including those related to experimental\\ndesign, data collection and selection (e.g., availability, repre-\\nsentativeness, suitability), system trustworthiness, and construct\\nvalidation.\\nMAP 3.1: Potential benefits of intended AI system functionality\\nand performance are examined and documented.\\nMAP 3.2: Potential costs, including non-monetary costs, which\\nresult from expected or realized AI errors or system functionality\\nand trustworthiness – as connected to organizational risk toler-\\nance – are examined and documented.\\nMAP 3.3: Targeted application scope is specified and docu-\\nmented based on the system’s capability, established context, and\\nAI system categorization.\\nMAP 3.4: Processes for operator and practitioner proficiency\\nwith AI system performance and trustworthiness – and relevant\\ntechnical standards and certifications – are defined, assessed, and\\ndocumented.\\nMAP 3.5: Processes for human oversight are defined, assessed,\\nand documented in accordance with organizational policies from\\nthe GOVERN function.\\nMAP 4.1: Approaches for mapping AI technology and legal risks\\nof its components – including the use of third-party data or soft-\\nware – are in place, followed, and documented, as are risks of in-\\nfringement of a third party’s intellectual property or other rights.\\nMAP 4.2: Internal risk controls for components of the AI sys-\\ntem, including third-party AI technologies, are identified and\\ndocumented.\\nMAP 5.1: Likelihood and magnitude of each identified impact\\n(both potentially beneficial and harmful) based on expected use,\\npast uses of AI systems in similar contexts, public incident re-\\nports, feedback from those external to the team that developed\\nor deployed the AI system, or other data are identified and\\ndocumented. Continued on next page Page 27 NIST AI 100-1 AI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued) Categories Subcategories\\nMAP 5.2: Practices and personnel for supporting regular en-\\ngagement with relevant AI actors and integrating feedback about\\npositive, negative, and unanticipated impacts are in place and\\ndocumented.', '5.3 Measure\\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-\\nniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related\\nimpacts. It uses knowledge relevant to AI risks identified in the MAP function and informs\\nthe MANAGE function. AI systems should be tested before their deployment and regu-\\nlarly while in operation. AI risk measurements include documenting aspects of systems’\\nfunctionality and trustworthiness. Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact,\\nand human-AI configurations. Processes developed or adopted in the MEASURE function\\nshould include rigorous software testing and performance assessment methodologies with\\nassociated measures of uncertainty, comparisons to performance benchmarks, and formal-\\nized reporting and documentation of results. Processes for independent review can improve\\nthe effectiveness of testing and can mitigate internal biases and potential conflicts of inter-\\nest. Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace-\\nable basis to inform management decisions. Options may include recalibration, impact\\nmitigation, or removal of the system from design, development, production, or use, as well\\nas a range of compensating, detective, deterrent, directive, and recovery controls. After completing the MEASURE function, objective, repeatable, or scalable test, evaluation,\\nverification, and validation (TEVV) processes including metrics, methods, and methodolo-\\ngies are in place, followed, and documented. Metrics and measurement methodologies\\nshould adhere to scientific, legal, and ethical norms and be carried out in an open and trans-\\nparent process. New types of measurement, qualitative and quantitative, may need to be\\ndeveloped. The degree to which each measurement type provides unique and meaningful\\ninformation to the assessment of AI risks should be considered. Framework users will en-\\nhance their capacity to comprehensively evaluate system trustworthiness, identify and track\\nexisting and emergent risks, and verify efficacy of the metrics. Measurement outcomes will\\nbe utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-\\ncumbent on Framework users to continue applying the MEASURE function to AI systems\\nas knowledge, methodologies, risks, and impacts evolve over time. Page 28 NIST AI 100-1 AI RMF 1.0 Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table\\n3 lists the MEASURE function’s categories and subcategories. Table 3: Categories and subcategories for the MEASURE function. Categories MEASURE 1:\\nAppropriate\\nmethods and metrics\\nare identified and\\napplied. MEASURE 2: AI\\nsystems are\\nevaluated for\\ntrustworthy\\ncharacteristics. Subcategories\\nMEASURE 1.1: Approaches and metrics for measurement of AI\\nrisks enumerated during the MAP function are selected for imple-\\nmentation starting with the most significant AI risks. The risks\\nor trustworthiness characteristics that will not – or cannot – be\\nmeasured are properly documented.\\nMEASURE 1.2: Appropriateness of AI metrics and effectiveness\\nof existing controls are regularly assessed and updated, including\\nreports of errors and potential impacts on affected communities.\\nMEASURE 1.3: Internal experts who did not serve as front-line\\ndevelopers for the system and/or independent assessors are in-\\nvolved in regular assessments and updates. Domain experts,\\nusers, AI actors external to the team that developed or deployed\\nthe AI system, and affected communities are consulted in support\\nof assessments as necessary per organizational risk tolerance.\\nMEASURE 2.1: Test sets, metrics, and details about the tools used\\nduring TEVV are documented.\\nMEASURE 2.2: Evaluations involving human subjects meet ap-\\nplicable requirements (including human subject protection) and\\nare representative of the relevant population.\\nMEASURE 2.3: AI system performance or assurance criteria\\nare measured qualitatively or quantitatively and demonstrated\\nfor conditions similar to deployment setting(s). Measures are\\ndocumented.\\nMEASURE 2.4: The functionality and behavior of the AI sys-\\ntem and its components – as identified in the MAP function – are\\nmonitored when in production.\\nMEASURE 2.5: The AI system to be deployed is demonstrated\\nto be valid and reliable. Limitations of the generalizability be-\\nyond the conditions under which the technology was developed\\nare documented. Continued on next page Page 29 NIST AI 100-1 AI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)\\nCategories MEASURE 3:\\nMechanisms for\\ntracking identified\\nAI risks over time\\nare in place. Subcategories\\nMEASURE 2.6: The AI system is evaluated regularly for safety\\nrisks – as identified in the MAP function. The AI system to be de-\\nployed is demonstrated to be safe, its residual negative risk does\\nnot exceed the risk tolerance, and it can fail safely, particularly if\\nmade to operate beyond its knowledge limits. Safety metrics re-\\nflect system reliability and robustness, real-time monitoring, and\\nresponse times for AI system failures.\\nMEASURE 2.7: AI system security and resilience – as identified\\nin the MAP function – are evaluated and documented.\\nMEASURE 2.8: Risks associated with transparency and account-\\nability – as identified in the MAP function – are examined and\\ndocumented.\\nMEASURE 2.9: The AI model is explained, validated, and docu-\\nmented, and AI system output is interpreted within its context –\\nas identified in the MAP function – to inform responsible use and\\ngovernance.\\nMEASURE 2.10: Privacy risk of the AI system – as identified in\\nthe MAP function – is examined and documented.\\nMEASURE 2.11: Fairness and bias – as identified in the MAP\\nfunction – are evaluated and results are documented.\\nMEASURE 2.12: Environmental impact and sustainability of AI\\nmodel training and management activities – as identified in the\\nMAP function – are assessed and documented.\\nMEASURE 2.13: Effectiveness of the employed TEVV met-\\nrics and processes in the MEASURE function are evaluated and\\ndocumented.\\nMEASURE 3.1: Approaches, personnel, and documentation are\\nin place to regularly identify and track existing, unanticipated,\\nand emergent AI risks based on factors such as intended and ac-\\ntual performance in deployed contexts.\\nMEASURE 3.2: Risk tracking approaches are considered for\\nsettings where AI risks are difficult to assess using currently\\navailable measurement techniques or where metrics are not yet\\navailable. Continued on next page Page 30 NIST AI 100-1 AI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)\\nCategories MEASURE 4:\\nFeedback about\\nefficacy of\\nmeasurement is\\ngathered and\\nassessed. Subcategories\\nMEASURE 3.3: Feedback processes for end users and impacted\\ncommunities to report problems and appeal system outcomes are\\nestablished and integrated into AI system evaluation metrics.\\nMEASURE 4.1: Measurement approaches for identifying AI risks\\nare connected to deployment context(s) and informed through\\nconsultation with domain experts and other end users. Ap-\\nproaches are documented.\\nMEASURE 4.2: Measurement results regarding AI system trust-\\nworthiness in deployment context(s) and across the AI lifecycle\\nare informed by input from domain experts and relevant AI ac-\\ntors to validate whether the system is performing consistently as\\nintended. Results are documented.\\nMEASURE 4.3: Measurable performance improvements or de-\\nclines based on consultations with relevant AI actors,\\nin-\\ncluding affected communities, and field data about context-\\nrelevant risks and trustworthiness characteristics are identified\\nand documented.', '5.4 Manage\\nThe MANAGE function entails allocating risk resources to mapped and measured risks on\\na regular basis and as defined by the GOVERN function. Risk treatment comprises plans to\\nrespond to, recover from, and communicate about incidents or events. Contextual information gleaned from expert consultation and input from relevant AI actors\\n– established in GOVERN and carried out in MAP – is utilized in this function to decrease\\nthe likelihood of system failures and negative impacts. Systematic documentation practices\\nestablished in GOVERN and utilized in MAP and MEASURE bolster AI risk management\\nefforts and increase transparency and accountability. Processes for assessing emergent risks\\nare in place, along with mechanisms for continual improvement. After completing the MANAGE function, plans for prioritizing risk and regular monitoring\\nand improvement will be in place. Framework users will have enhanced capacity to man-\\nage the risks of deployed AI systems and to allocate risk management resources based on\\nassessed and prioritized risks. It is incumbent on Framework users to continue to apply\\nthe MANAGE function to deployed AI systems as methods, contexts, risks, and needs or\\nexpectations from relevant AI actors evolve over time. Page 31 NIST AI 100-1 AI RMF 1.0 Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table\\n4 lists the MANAGE function’s categories and subcategories. Table 4: Categories and subcategories for the MANAGE function. Categories\\nMANAGE 1: AI\\nrisks based on\\nassessments and\\nother analytical\\noutput from the\\nMAP and MEASURE\\nfunctions are\\nprioritized,\\nresponded to, and\\nmanaged. MANAGE 2:\\nStrategies to\\nmaximize AI\\nbenefits and\\nminimize negative\\nimpacts are planned,\\nprepared,\\nimplemented,\\ndocumented, and\\ninformed by input\\nfrom relevant AI\\nactors. MANAGE 3: AI\\nrisks and benefits\\nfrom third-party\\nentities are\\nmanaged. Subcategories\\nMANAGE 1.1: A determination is made as to whether the AI\\nsystem achieves its intended purposes and stated objectives and\\nwhether its development or deployment should proceed.\\nMANAGE 1.2: Treatment of documented AI risks is prioritized\\nbased on impact, likelihood, and available resources or methods.\\nMANAGE 1.3: Responses to the AI risks deemed high priority, as\\nidentified by the MAP function, are developed, planned, and doc-\\numented. Risk response options can include mitigating, transfer-\\nring, avoiding, or accepting.\\nMANAGE 1.4: Negative residual risks (defined as the sum of all\\nunmitigated risks) to both downstream acquirers of AI systems\\nand end users are documented.\\nMANAGE 2.1: Resources required to manage AI risks are taken\\ninto account – along with viable non-AI alternative systems, ap-\\nproaches, or methods – to reduce the magnitude or likelihood of\\npotential impacts.\\nMANAGE 2.2: Mechanisms are in place and applied to sustain\\nthe value of deployed AI systems.\\nMANAGE 2.3: Procedures are followed to respond to and recover\\nfrom a previously unknown risk when it is identified.\\nMANAGE 2.4: Mechanisms are in place and applied, and respon-\\nsibilities are assigned and understood, to supersede, disengage, or\\ndeactivate AI systems that demonstrate performance or outcomes\\ninconsistent with intended use.\\nMANAGE 3.1: AI risks and benefits from third-party resources\\nare regularly monitored, and risk controls are applied and\\ndocumented.\\nMANAGE 3.2: Pre-trained models which are used for develop-\\nment are monitored as part of AI system regular monitoring and\\nmaintenance. Continued on next page Page 32 NIST AI 100-1 AI RMF 1.0 Table 4: Categories and subcategories for the MANAGE function. (Continued)\\nCategories MANAGE 4: Risk\\ntreatments,\\nincluding response\\nand recovery, and\\ncommunication\\nplans for the\\nidentified and\\nmeasured AI risks\\nare documented and\\nmonitored regularly. Subcategories\\nMANAGE 4.1: Post-deployment AI system monitoring plans\\nare implemented, including mechanisms for capturing and eval-\\nuating input from users and other relevant AI actors, appeal\\nand override, decommissioning, incident response, recovery, and\\nchange management.\\nMANAGE 4.2: Measurable activities for continual improvements\\nare integrated into AI system updates and include regular engage-\\nment with interested parties, including relevant AI actors.\\nMANAGE 4.3: Incidents and errors are communicated to relevant\\nAI actors, including affected communities. Processes for track-\\ning, responding to, and recovering from incidents and errors are\\nfollowed and documented.', '6. AI RMF Profiles\\nAppendix A: Descriptions of AI Actor Tasks from Figures 2 and 3\\nAppendix B: How AI Risks Differ from Traditional Software Risks\\nAppendix C: AI Risk Management and Human-AI Interaction\\nAppendix D: Attributes of the AI RMF List of Tables Table 1 Categories and subcategories for the GOVERN function.\\nTable 2 Categories and subcategories for the MAP function.\\nTable 3 Categories and subcategories for the MEASURE function.\\nTable 4 Categories and subcategories for the MANAGE function. i 1\\n4\\n4\\n4\\n5\\n5\\n7\\n7\\n8\\n9\\n12\\n13\\n14\\n15\\n15\\n16\\n17\\n17\\n19\\n20\\n20\\n21\\n24\\n28\\n31\\n33\\n35\\n38\\n40\\n42 22\\n26\\n29\\n32 NIST AI 100-1 AI RMF 1.0 List of Figures 5 10 11 12 20 Page ii NIST AI 100-1 AI RMF 1.0 AI RMF use-case profiles are implementations of the AI RMF functions, categories, and\\nsubcategories for a specific setting or application based on the requirements, risk tolerance,\\nand resources of the Framework user: for example, an AI RMF hiring profile or an AI\\nRMF fair housing profile. Profiles may illustrate and offer insights into how risk can be\\nmanaged at various stages of the AI lifecycle or in specific sector, technology, or end-use\\napplications. AI RMF profiles assist organizations in deciding how they might best manage\\nAI risk that is well-aligned with their goals, considers legal/regulatory requirements and\\nbest practices, and reflects risk management priorities. AI RMF temporal profiles are descriptions of either the current state or the desired, target\\nstate of specific AI risk management activities within a given sector, industry, organization,\\nor application context. An AI RMF Current Profile indicates how AI is currently being\\nmanaged and the related risks in terms of current outcomes. A Target Profile indicates the\\noutcomes needed to achieve the desired or target AI risk management goals. Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk\\nmanagement objectives. Action plans can be developed to address these gaps to fulfill\\noutcomes in a given category or subcategory. Prioritization of gap mitigation is driven by\\nthe user’s needs and risk management processes. This risk-based approach also enables\\nFramework users to compare their approaches with other approaches and to gauge the\\nresources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-\\neffective, prioritized manner. Page 33 NIST AI 100-1 AI RMF 1.0 AI RMF cross-sectoral profiles cover risks of models or applications that can be used across\\nuse cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure,\\nand manage risks for activities or business processes common across sectors such as the\\nuse of large language models, cloud-based services or acquisition. This Framework does not prescribe profile templates, allowing for flexibility in implemen-\\ntation. Page 34 NIST AI 100-1 AI RMF 1.0', 'Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3\\nDescriptions of AI Actor Tasks from Figures 2 and 3\\nAI Design tasks are performed during the Application Context and Data and Input phases\\nof the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI\\nsystems and are responsible for the planning, design, and data collection and processing\\ntasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-\\nticulating and documenting the system’s concept and objectives, underlying assumptions,\\ncontext, and requirements; gathering and cleaning data; and documenting the metadata\\nand characteristics of the dataset. AI actors in this category include data scientists, do-\\nmain experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion,\\nand accessibility, members of impacted communities, human factors experts (e.g., UX/UI\\ndesign), governance experts, data engineers, data providers, system funders, product man-\\nagers, third-party entities, evaluators, and legal and privacy governance. AI Development tasks are performed during the AI Model phase of the lifecycle in Figure\\n2. AI Development actors provide the initial infrastructure of AI systems and are responsi-\\nble for model building and interpretation tasks, which involve the creation, selection, cali-\\nbration, training, and/or testing of models or algorithms. AI actors in this category include\\nmachine learning experts, data scientists, developers, third-party entities, legal and privacy\\ngovernance experts, and experts in the socio-cultural and contextual factors associated with\\nthe deployment setting. AI Deployment tasks are performed during the Task and Output phase of the lifecycle in\\nFigure 2. AI Deployment actors are responsible for contextual decisions relating to how\\nthe AI system is used to assure deployment of the system into production. Related tasks\\ninclude piloting the system, checking compatibility with legacy systems, ensuring regu-\\nlatory compliance, managing organizational change, and evaluating user experience. AI\\nactors in this category include system integrators, software developers, end users, oper-\\nators and practitioners, evaluators, and domain experts with expertise in human factors,\\nsocio-cultural analysis, and governance. Operation and Monitoring tasks are performed in the Application Context/Operate and\\nMonitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are\\nresponsible for operating the AI system and working with others to regularly assess system\\noutput and impacts. AI actors in this category include system operators, domain experts, AI\\ndesigners, users who interpret or incorporate the output of AI systems, product developers,\\nevaluators and auditors, compliance experts, organizational management, and members of\\nthe research community. Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout\\nthe AI lifecycle. They are carried out by AI actors who examine the AI system or its\\ncomponents, or detect and remediate problems. Ideally, AI actors carrying out verification Page 35 NIST AI 100-1 AI RMF 1.0 and validation tasks are distinct from those who perform test and evaluation actions. Tasks\\ncan be incorporated into a phase as early as design, where tests are planned in accordance\\nwith the design requirement. • TEVV tasks for design, planning, and data may center on internal and external vali-\\ndation of assumptions for system design, data collection, and measurements relative\\nto the intended context of deployment or application. • TEVV tasks for development (i.e., model building) include model validation and assessment. • TEVV tasks for deployment include system validation and integration in production,\\nwith testing, and recalibration for systems and process integration, user experience,\\nand compliance with existing legal, regulatory, and ethical specifications. • TEVV tasks for operations involve ongoing monitoring for periodic updates, testing,\\nand subject matter expert (SME) recalibration of models, the tracking of incidents\\nor errors reported and their management, the detection of emergent properties and\\nrelated impacts, and processes for redress and response. Human Factors tasks and activities are found throughout the dimensions of the AI life-\\ncycle. They include human-centered design practices and methodologies, promoting the\\nactive involvement of end users and other interested parties and relevant AI actors, incor-\\nporating context-specific norms and values in system design, evaluating and adapting end\\nuser experiences, and broad integration of humans and human dynamics in all phases of the\\nAI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives\\nto understand context of use, inform interdisciplinary and demographic diversity, engage\\nin consultative processes, design and evaluate user experience, perform human-centered\\nevaluation and testing, and inform impact assessments. Domain Expert tasks involve input from multidisciplinary practitioners or scholars who\\nprovide knowledge or expertise in – and about – an industry sector, economic sector, con-\\ntext, or application area where an AI system is being used. AI actors who are domain\\nexperts can provide essential guidance for AI system design and development, and inter-\\npret outputs in support of work performed by TEVV and AI impact assessment teams. AI Impact Assessment tasks include assessing and evaluating requirements for AI system\\naccountability, combating harmful bias, examining impacts of AI systems, product safety,\\nliability, and security, among others. AI actors such as impact assessors and evaluators\\nprovide technical, human factor, socio-cultural, and legal expertise. Procurement tasks are conducted by AI actors with financial, legal, or policy management\\nauthority for acquisition of AI models, products, or services from a third-party developer,\\nvendor, or contractor. Governance and Oversight tasks are assumed by AI actors with management, fiduciary,\\nand legal authority and responsibility for the organization in which an AI system is de- Page 36 NIST AI 100-1 AI RMF 1.0 signed, developed, and/or deployed. Key AI actors responsible for AI governance include\\norganizational management, senior leadership, and the Board of Directors. These actors\\nare parties that are concerned with the impact and sustainability of the organization as a\\nwhole. Additional AI Actors Third-party entities include providers, developers, vendors, and evaluators of data, al-\\ngorithms, models, and/or systems and related services for another organization or the or-\\nganization’s customers or clients. Third-party entities are responsible for AI design and\\ndevelopment tasks, in whole or in part. By definition, they are external to the design, devel-\\nopment, or deployment team of the organization that acquires its technologies or services.\\nThe technologies acquired from third-party entities may be complex or opaque, and risk\\ntolerances may not align with the deploying or operating organization. End users of an AI system are the individuals or groups that use the system for specific\\npurposes. These individuals or groups interact with an AI system in a specific context. End\\nusers can range in competency from AI experts to first-time technology end users. Affected individuals/communities encompass all individuals, groups, communities, or\\norganizations directly or indirectly affected by AI systems or decisions based on the output\\nof AI systems. These individuals do not necessarily interact with the deployed system or\\napplication. Other AI actors may provide formal or quasi-formal norms or guidance for specifying\\nand managing AI risks. They can include trade associations, standards developing or-\\nganizations, advocacy groups, researchers, environmental groups, and civil society\\norganizations. The general public is most likely to directly experience positive and negative impacts of\\nAI technologies. They may provide the motivation for actions taken by the AI actors. This\\ngroup can include individuals, communities, and consumers associated with the context in\\nwhich an AI system is developed or deployed. Page 37 NIST AI 100-1 AI RMF 1.0', 'Appendix B: How AI Risks Differ from Traditional Software Risks\\nHow AI Risks Differ from Traditional Software Risks\\nAs with traditional software, risks from AI-based technology can be bigger than an en-\\nterprise, span organizations, and lead to societal impacts. AI systems also bring a set of\\nrisks that are not comprehensively addressed by current risk frameworks and approaches.\\nSome AI system features that present risks also can be beneficial. For example, pre-trained\\nmodels and transfer learning can advance research and increase accuracy and resilience\\nwhen compared to other models and approaches. Identifying contextual factors in the MAP\\nfunction will assist AI actors in determining the level of risk and potential management\\nefforts. Compared to traditional software, AI-specific risks that are new or increased include the\\nfollowing: • The data used for building an AI system may not be a true or appropriate representa-\\ntion of the context or intended use of the AI system, and the ground truth may either\\nnot exist or not be available. Additionally, harmful bias and other data quality issues\\ncan affect AI system trustworthiness, which could lead to negative impacts. • AI system dependency and reliance on data for training tasks, combined with in- creased volume and complexity typically associated with such data. • Intentional or unintentional changes during training may fundamentally alter AI sys- tem performance. • Datasets used to train AI systems may become detached from their original and in- tended context or may become stale or outdated relative to deployment context. • AI system scale and complexity (many systems contain billions or even trillions of decision points) housed within more traditional software applications. • Use of pre-trained models that can advance research and improve performance can\\nalso increase levels of statistical uncertainty and cause issues with bias management,\\nscientific validity, and reproducibility. • Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. • Privacy risk due to enhanced data aggregation capability for AI systems.\\n• AI systems may require more frequent maintenance and triggers for conducting cor- rective maintenance due to data, model, or concept drift. • Increased opacity and concerns about reproducibility.\\n• Underdeveloped software testing standards and inability to document AI-based prac-\\ntices to the standard expected of traditionally engineered software for all but the\\nsimplest of cases. • Difficulty in performing regular AI-based software testing, or determining what to\\ntest, since AI systems are not subject to the same controls as traditional code devel-\\nopment. Page 38 NIST AI 100-1 AI RMF 1.0 • Computational costs for developing AI systems and their impact on the environment and planet. • Inability to predict or detect the side effects of AI-based systems beyond statistical measures. Privacy and cybersecurity risk management considerations and approaches are applicable\\nin the design, development, deployment, evaluation, and use of AI systems. Privacy and\\ncybersecurity risks are also considered as part of broader enterprise risk management con-\\nsiderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-\\nness characteristics such as “Secure and Resilient” and “Privacy-Enhanced,” organizations\\nmay consider leveraging available standards and guidance that provide broad guidance to\\norganizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-\\nbersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-\\nwork, and the Secure Software Development Framework. These frameworks have some\\nfeatures in common with the AI RMF. Like most risk management approaches, they are\\noutcome-based rather than prescriptive and are often structured around a Core set of func-\\ntions, categories, and subcategories. While there are significant differences between these\\nframeworks based on the domain addressed – and because AI risk management calls for\\naddressing many other types of risks – frameworks like those mentioned above may inform\\nsecurity and privacy considerations in the MAP, MEASURE, and MANAGE functions of the\\nAI RMF. At the same time, guidance available before publication of this AI RMF does not compre-\\nhensively address many AI system risks. For example, existing frameworks and guidance\\nare unable to: • adequately manage the problem of harmful bias in AI systems;\\n• confront the challenging risks related to generative AI;\\n• comprehensively address security concerns related to evasion, model extraction, mem- bership inference, availability, or other machine learning attacks; • account for the complex attack surface of AI systems or other security abuses enabled by AI systems; and • consider risks associated with third-party AI technologies, transfer learning, and off-\\nlabel use where AI systems may be trained for decision-making outside an organiza-\\ntion’s security controls or trained in one domain and then “fine-tuned” for another. Both AI and traditional software technologies and systems are subject to rapid innovation.\\nTechnology advances should be monitored and deployed to take advantage of those devel-\\nopments and work towards a future of AI that is both trustworthy and responsible. Page 39 NIST AI 100-1 AI RMF 1.0', 'Appendix C: AI Risk Management and Human-AI Interaction\\nAI Risk Management and Human-AI Interaction\\nOrganizations that design, develop, or deploy AI systems for use in operational settings\\nmay enhance their AI risk management by understanding current limitations of human-\\nAI interaction. The AI RMF provides opportunities to clearly define and differentiate the\\nvarious human roles and responsibilities when using, interacting with, or managing AI\\nsystems. Many of the data-driven approaches that AI systems rely on attempt to convert or represent\\nindividual and social observational and decision-making practices into measurable quanti-\\nties. Representing complex human phenomena with mathematical models can come at the\\ncost of removing necessary context. This loss of context may in turn make it difficult to\\nunderstand individual and societal impacts that are key to AI risk management efforts. Issues that merit further consideration and research include:', 'Appendix D: Attributes of the AI RMF\\nAttributes of the AI RMF\\nNIST described several key attributes of the AI RMF when work on the Framework first\\nbegan. These attributes have remained intact and were used to guide the AI RMF’s devel-\\nopment. They are provided here as a reference. The AI RMF strives to:']\n"
     ]
    }
   ],
   "source": [
    "json_data = read_json_file('AI__json_output_2024-12-05_12-59-34.json')\n",
    "\n",
    "sections = []\n",
    "\n",
    "heading_title = ''\n",
    "\n",
    "for section in json_data['sections']:\n",
    "    heading_title = section['heading']\n",
    "    paragraphs = \" \".join(section['paragraphs'])\n",
    "    sections.append(f'{heading_title}\\n{paragraphs}')\n",
    "\n",
    "\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55ab659-29cd-4ef3-b752-c5a25fadac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "#Split Text using Regular Expressions\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "#Open text file\n",
    "\n",
    "# Open a file using a context manager\n",
    "with open('AI_RMP_extract4.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    #print(content)\n",
    "\n",
    "# Sample text simulating the content of a text file\n",
    "\n",
    "text = \"\"\"\n",
    "Page 1\n",
    "This is the content of page 1.\n",
    "It has multiple lines.\n",
    "\n",
    "Page 2\n",
    "This is the content of page 2.\n",
    "It also has multiple lines.\n",
    "\n",
    "Page 3\n",
    "This is the content of page 3.\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to match 'Page #' where # is one or more digits\n",
    "#pattern = r'Page \\d+'\n",
    "pattern = r'Page (\\d+|[ivxlcdm]+)'\n",
    "\n",
    "# Split the text using the pattern\n",
    "pages = re.split(pattern, content)\n",
    "\n",
    "# Remove any empty strings from the list\n",
    "sections = [page.strip() for page in pages if page.strip()]\n",
    "\n",
    "print(len(sections))\n",
    "\n",
    "# Print the pages\n",
    "\n",
    "#for i, page in enumerate(pages, start=1):\n",
    "#    print(f\"Page {i}:\\n{page}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7149d1ba-a7c9-4d40-b7d4-f6454edfc1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/usr/local/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model for generating embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each section\n",
    "section_embeddings = model.encode(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2c288c-52a2-45a7-af87-c6febc7c9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections uploaded to Pinecone!\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "os.environ['PINECONE_API_KEY'] = '13707c0e-f5d1-4eaa-8c72-4988914c1de3'\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ.get(\"PINECONE_API_KEY\")\n",
    ")\n",
    "index_name = 'pdf-sections-12062024'\n",
    "# Now do stuff\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name, \n",
    "        dimension=384, \n",
    "        metric='euclidean',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "# Initialize Pinecone\n",
    "#pinecone.init(api_key=\"13707c0e-f5d1-4eaa-8c72-4988914c1de3\", environment=\"us-east-1\")\n",
    "\n",
    "# Create a new index\n",
    "#index_name = 'pdf_sections'\n",
    "#pinecone.create_index(index_name, dimension=768)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Upload section embeddings\n",
    "for i, embedding in enumerate(section_embeddings):\n",
    "    index.upsert([(str(i), embedding)])\n",
    "\n",
    "print(\"Sections uploaded to Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35383a2a-d71f-40ed-b1b7-dd6f7335f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "11\n",
      "17\n",
      "4\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Search for a query within the document sections\n",
    "query = \"Trustworthiness\"\n",
    "query_embedding = model.encode([query])\n",
    "#print(query_embedding[0])\n",
    "query_embedding_list = query_embedding[0].tolist()\n",
    "\n",
    "# Search the vector database\n",
    "#search_results = index.query(vector=query_embedding[0], top_k=5)\n",
    "search_results = index.query(\n",
    "    \n",
    "    vector=query_embedding_list,\n",
    "    top_k=5,\n",
    "    include_values=True\n",
    ")\n",
    "#print(search_results)\n",
    "# Retrieve and display the most relevant sections\n",
    "for match in search_results['matches']:\n",
    "    section_id = int(match['id'])\n",
    "    #print(f\"Section {section_id}: {sections[section_id]}\")\n",
    "    print(section_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "440f443d-fea4-4720-aacf-b0abd7f64d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(query):\n",
    "    # Search for a query within the document sections\n",
    "    query = \"Trustworthiness\"\n",
    "    query_embedding = model.encode([query])\n",
    "    #print(query_embedding[0])\n",
    "    query_embedding_list = query_embedding[0].tolist()\n",
    "    \n",
    "    # Search the vector database\n",
    "    #search_results = index.query(vector=query_embedding[0], top_k=5)\n",
    "    search_results = index.query(\n",
    "        \n",
    "        vector=query_embedding_list,\n",
    "        top_k=5,\n",
    "        include_values=True\n",
    "    )\n",
    "\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24757185-7fd6-4d34-b2ad-481b2337c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document saved to JSON!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example structure for storing sections and subsections\n",
    "document_json = {\n",
    "    \"title\": \"Extracted Document\",\n",
    "    \"sections\": []\n",
    "}\n",
    "\n",
    "# Append sections to the JSON object\n",
    "for match in search_results['matches']:\n",
    "    section_id = int(match['id'])\n",
    "    document_json[\"sections\"].append({\n",
    "        \"section_id\": section_id,\n",
    "        \"content\": sections[section_id]\n",
    "    })\n",
    "\n",
    "# Save the structured data to a JSON file\n",
    "with open('PDF_document2.json', 'w') as json_file:\n",
    "    json.dump(document_json, json_file, indent=2)\n",
    "\n",
    "print(\"Document saved to JSON!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "387ba974-c22c-4263-9452-5c4d9affccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_file(keyword, search_results):\n",
    "    # Example structure for storing sections and subsections\n",
    "    document_json = {\n",
    "        \"title\": f\"Extracted document for keyword {keyword}\",\n",
    "        \"sections\": []\n",
    "    }\n",
    "\n",
    "    if(search_results):\n",
    "        # Append sections to the JSON object\n",
    "        for match in search_results['matches']:\n",
    "            section_id = int(match['id'])\n",
    "            document_json[\"sections\"].append({\n",
    "                \"section_id\": section_id,\n",
    "                \"content\": sections[section_id]\n",
    "            })\n",
    "    \n",
    "        # Remove spaces and set all characters to lower case \n",
    "        modified_string = keyword.replace(\" \", \"\").lower()\n",
    "        file_name = f'query_results_{modified_string}.json'\n",
    "        # Save the structured data to a JSON file\n",
    "        with open(file_name, 'w') as json_file:\n",
    "            json.dump(document_json, json_file, indent=2)\n",
    "        \n",
    "        print(f\"{file_name} saved to JSON!\")\n",
    "    else:\n",
    "        print(\"No search results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d09bb8c-ac08-4f14-aa16-4f6459b3e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "#Create db driver ...\n",
    "\n",
    "# URI examples: \"neo4j://localhost\", \"neo4j+s://xxx.databases.neo4j.io\"\n",
    "URI = \"neo4j+s://1eeb71d4.databases.neo4j.io\"\n",
    "AUTH = (\"neo4j\", \"ry8qCNwcZajMLLzq_fmzZER6DYcn3JMKfSTY0-HpZlw\")\n",
    "\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d44712a-b063-4a65-b7c4-d0aaa777e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/gs7hmkc56sl9k2z0v7pgmlw00000gn/T/ipykernel_6234/3765920612.py:7: DeprecationWarning: Using a driver after it has been closed is deprecated. Future versions of the driver will raise an error.\n",
      "  records, summary, keys = driver.execute_query(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_results_aiethics.json saved to JSON!\n",
      "query_results_transparency.json saved to JSON!\n",
      "query_results_accountability.json saved to JSON!\n",
      "query_results_biasmitigation.json saved to JSON!\n",
      "query_results_fairness.json saved to JSON!\n",
      "query_results_dataprivacy.json saved to JSON!\n",
      "query_results_dataprotection.json saved to JSON!\n",
      "query_results_regulatorycompliance.json saved to JSON!\n",
      "query_results_gdpr(generaldataprotectionregulation).json saved to JSON!\n",
      "query_results_audittrails.json saved to JSON!\n",
      "query_results_explainability.json saved to JSON!\n",
      "query_results_robustness.json saved to JSON!\n",
      "query_results_security.json saved to JSON!\n",
      "query_results_humanoversight.json saved to JSON!\n",
      "query_results_datagovernance.json saved to JSON!\n",
      "query_results_riskmanagement.json saved to JSON!\n",
      "query_results_ethicalai.json saved to JSON!\n",
      "query_results_algorithmicaccountability.json saved to JSON!\n",
      "query_results_modelmonitoring.json saved to JSON!\n",
      "query_results_complianceframework.json saved to JSON!\n",
      "query_results_stakeholderengagement.json saved to JSON!\n",
      "query_results_responsibleai.json saved to JSON!\n",
      "query_results_trustworthiness.json saved to JSON!\n",
      "query_results_informedconsent.json saved to JSON!\n",
      "query_results_impactassessment.json saved to JSON!\n",
      "query_results_diversityinai.json saved to JSON!\n",
      "query_results_vendorcompliance.json saved to JSON!\n",
      "query_results_usagepolicies.json saved to JSON!\n",
      "query_results_incidentresponse.json saved to JSON!\n",
      "query_results_regulatorylandscape.json saved to JSON!\n",
      "query_results_benchmarking.json saved to JSON!\n",
      "query_results_performancemetrics.json saved to JSON!\n",
      "query_results_adaptivecompliance.json saved to JSON!\n",
      "query_results_dataminimization.json saved to JSON!\n",
      "query_results_culturalsensitivity.json saved to JSON!\n",
      "query_results_userrights.json saved to JSON!\n",
      "query_results_algorithmbias.json saved to JSON!\n",
      "query_results_trainingdataaccountability.json saved to JSON!\n",
      "query_results_policydevelopment.json saved to JSON!\n",
      "query_results_policydevelopment.json saved to JSON!\n",
      "query_results_legalliability.json saved to JSON!\n",
      "query_results_innovativeregulation.json saved to JSON!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the graph\n",
    "query = \"\"\"\n",
    " MATCH (k:Keyword)\n",
    " RETURN k.name AS Keyword\n",
    "\"\"\"\n",
    "\n",
    "records, summary, keys = driver.execute_query(\n",
    "    query,\n",
    "    keyword='Trustworthiness',\n",
    "    database_=\"neo4j\",\n",
    ")\n",
    "driver.close()\n",
    "# Loop through results and do something with them\n",
    "for record in records:\n",
    "    keyword = record['Keyword']\n",
    "    results = get_search_results(keyword)\n",
    "    output_to_file(keyword, results)\n",
    "    #print(record['Keyword'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784860a-5f4c-4d2a-895b-d4aa0be0b0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
